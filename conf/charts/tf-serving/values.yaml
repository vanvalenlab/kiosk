# Default values for tf-serving.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicas: 0

image:
  repository: vanvalenlab/kiosk-tf-serving
  tag: latest
  pullPolicy: IfNotPresent

service:
  type: ClusterIP

  httpIngressEnabled: true
  internalHttpPort: 8501
  httpTargetPort: 8501
  externalHttpPort: 8501

  grpcIngressEnabled: true
  internalGrpcPort: 8500
  grpcTargetPort: 8500
  externalGrpcPort: 8500

  httpsIngressEnabled: false

nodeSelector: {}
  #  cloud.google.com/gke-accelerator: "nvidia-tesla-k80"
  #  beta.kubernetes.io/instance-type: "p2.xlarge"
  #  cloud.google.com/gke-preemptible: "true"

resources: {}
  # requests:
  #   nvidia.com/gpu: 1
  # limits:
  #   nvidia.com/gpu: 1

tolerations: {}
# - key: "nvidia.com/gpu"
#   operator: "Exists"
#   effect: "NoSchedule"
# - key: "prediction_gpu"
#   operator: "Exists"
#   effect: "NoSchedule"

affinity: {}

env:
  PORT: 8500
  REST_API_PORT: 8501
  REST_API_TIMEOUT: 30000
  MODEL_CONFIG_FILE: /kiosk/tf-serving/models.conf
  BATCHING_CONFIG_FILE: /kiosk/tf-serving/batching_config.txt
  ENABLE_BATCHING: "true"
  MAX_BATCH_SIZE: 1
  BATCH_TIMEOUT_MICROS: 0
  MAX_ENQUEUED_BATCHES: 512
  GRPC_CHANNEL_ARGS: ""
  MODEL_PREFIX: models
  CLOUD_PROVIDER: '{{ env "CLOUD_PROVIDER" | default "aws" }}'
  TF_CPP_MIN_LOG_LEVEL: 0
  TF_SESSION_PARALLELISM: 0
  MONITORING_CONFIG_FILE: /kiosk/tf-serving/monitoring_config.txt
  PROMETHEUS_MONITORING_ENABLED: "true"
  PROMETHEUS_MONITORING_PATH: /monitoring/prometheus/metrics

secrets:
  AWS_ACCESS_KEY_ID: "change_me"
  AWS_SECRET_ACCESS_KEY: "change_me"
  AWS_S3_BUCKET: "change_me"
  GCLOUD_STORAGE_BUCKET: "change_me"
