helmDefaults:
  wait: true
  timeout: 600
  force: true

releases:

#######################################################################################
## prometheus-operator                                                               ##
## creates/configures/manages Prometheus clusters atop Kubernetes                    ##
#######################################################################################

#
# References:
#   - https://github.com/coreos/prometheus-operator/tree/master/helm/prometheus-operator
#   - https://github.com/coreos/prometheus-operator
#
- name: prometheus-operator
  namespace: monitoring
  labels:
    chart: prometheus-operator
    repo: stable
    component: monitoring
    namespace: monitoring
    vendor: coreos
    default: true
  chart: stable/prometheus-operator
  version: 8.12.3
  wait: true
  values:
    # A list of all possible values can be found:
    # https://github.com/helm/charts/blob/master/stable/prometheus-operator/values.yaml
    - alertmanager:
        enabled: {{ env "ALERT_MANAGER_ENABLED" | default "true"}}
        config:
          global:
            resolve_timeout: 5m
          route:
            group_by: ['alertname', 'job']
            group_wait: 30s
            group_interval: 5m
            repeat_interval: 3h
            {{ if empty (env "ALERT_MANAGER_SLACK_API_URL" | default "") }}
            receiver: 'null'
            {{ else}}
            receiver: slack-receiver
            {{ end }}
            routes:
            - match_re:
                severity: error|warning|critical
              {{ if empty (env "ALERT_MANAGER_SLACK_API_URL" | default "") }}
              receiver: 'null'
              {{ else}}
              receiver: slack-receiver
              {{ end }}
              continue: true

          receivers:
          - name: 'null'
          {{ if (env "ALERT_MANAGER_SLACK_API_URL") }}
          - name: slack-receiver
            slack_configs:
            - api_url: '{{ env "ALERT_MANAGER_SLACK_API_URL" | default "" }}'
              channel: '{{ env "ALERT_MANAGER_SLACK_CHANNEL" | default "alerts" }}'
              send_resolved: true
              username: '{{`{{ template "slack.default.username" . }}`}}'
              color: '{{`{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}`}}'
              # title: '{{`{{ template "slack.custom.title" . }}`}}'
              title_link: '{{`{{ template "slack.default.titlelink" . }}`}}'
              pretext: '{{`{{ .CommonAnnotations.summary }}`}}'
              fallback: '{{`{{ template "slack.default.fallback" . }}`}}'
              icon_emoji: '{{`{{ template "slack.default.iconemoji" . }}`}}'
              icon_url: https://avatars3.githubusercontent.com/u/3380462
              # text: '{{`{{ template "slack.custom.text" . }}`}}'
              title: |-
                {{`[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
                {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
                  {{" "}}(
                  {{- with .CommonLabels.Remove .GroupLabels.Names }}
                    {{- range $index, $label := .SortedPairs -}}
                      {{ if $index }}, {{ end }}
                      {{- $label.Name }}="{{ $label.Value -}}"
                    {{- end }}
                  {{- end -}}
                  )
                {{- end }}`}}
              text: |-
                {{`{{ with index .Alerts 0 -}}
                  :chart_with_upwards_trend: *<{{ .GeneratorURL }}|Graph>*{{- if .Annotations.runbook_url }}   :notebook: *<{{ .Annotations.runbook_url }}|Runbook>*{{ end }}{{- if .Labels.severity }}   `}}`{{`{{ .Labels.severity }}`}}`{{`{{ end }}
                {{ end }}
                {{ range .Alerts -}}
                  {{- if .Annotations.message }}{{ .Annotations.message }}{{ end }}{{ if .Annotations.summary }}{{ .Annotations.summary }}
                  {{ end }}
                {{ end }}`}}
          {{ end }}
      additionalPrometheusRulesMap:
      - name: custom-prometheus-rules
        groups:
          - name: tf-serving-metrics
            rules:
            - record: tf_serving_gpu_usage
              expr: |-
                avg(
                  container_accelerator_duty_cycle{container_name="tf-serving"}
                ) or vector(0)
              labels:
                deployment: tf-serving
                namespace: deepcell
            - record: tf_serving_up
              expr: |-
                max(
                  clamp_max(
                    kube_deployment_status_replicas_available{deployment="tf-serving"},
                    1
                  ) or vector(0)
                )
              labels:
                deployment: tf-serving
                namespace: deepcell
          - name: consumer-metrics
            rules:
            - record: consumers_per_gpu
              expr: |-
                kube_deployment_status_replicas_available{deployment=~".*-consumer"}
                / on() group_left
                max(
                  kube_deployment_status_replicas_available{deployment="tf-serving"}
                  or vector(1)
                )
              labels:
                namespace: deepcell
            - record: consumer_key_ratio
              expr: |-
                redis_script_values
                / on(deployment)
                kube_deployment_spec_replicas
              labels:
                namespace: deepcell
            - record: segmentation_consumer_key_ratio
              # COMPLICATED scaling metric that prevents too many consumers
              # per GPU.  If too many consumers, scale down slightly.
              # Structural outline follows:
              # (
              #   min(1 - GPU, keys/consumers) * !is_too_many_consumers
              #   +
              #   .75 * target * is_too_many_consumers)
              # ) * is_tf_up
              expr: |-
                (
                  (
                    min(
                      1 - tf_serving_gpu_usage / 100
                      < on(namespace)
                      consumer_key_ratio{deployment="segmentation-consumer"} / 100
                      or
                      clamp_max(
                        consumer_key_ratio{deployment="segmentation-consumer"} / 100
                      , 1)
                    ) *
                    scalar(consumers_per_gpu{deployment="segmentation-consumer"} <= bool 150)
                  ) + (
                    scalar(consumers_per_gpu{deployment="segmentation-consumer"} > bool 150)
                  ) * .75 * .15
                ) * scalar(tf_serving_up > bool 0)
              labels:
                deployment: segmentation-consumer
                namespace: deepcell
            - record: segmentation_zip_consumer_key_ratio
              expr: |-
                consumer_key_ratio{deployment="segmentation-zip-consumer"}
              labels:
                deployment: zip-consumer
                namespace: deepcell
            - record: tracking_consumer_key_ratio
              expr: |-
                consumer_key_ratio{deployment="tracking-consumer"}
                * on() tf_serving_up
              labels:
                deployment: tracking-consumer
                namespace: deepcell


        ## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
        ##
        grafana:

          enabled: true

          ## Deploy default dashboards.
          ##
          defaultDashboardsEnabled: true

          adminPassword: prom-operator

          dashboards:
            default:
              prometheus-stats:
                # Ref: https://grafana.com/dashboards/2
                gnetId: 2
                revision: 2
                datasource: Prometheus
              prometheus-redis:
                # Ref: https://grafana.com/dashboards/763
                gnetId: 763
                revision: 2
                datasource: Prometheus

        ## Deploy a Prometheus instance
        ##
        prometheus:

          ## Settings affecting prometheusSpec
          ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
          ##
          prometheusSpec:

            ## Interval between consecutive scrapes.
            ##
            scrapeInterval: 15s

            ## Interval between consecutive evaluations.
            ##
            evaluationInterval: 15s

            ## Resource limits & requests
            ##
            # resources:
            #   requests:
            #     memory: 1Gi
            #   limits:
            #     memory: 1Gi

            ## Enable compression of the write-ahead log using Snappy.
            ##
            walCompression: true

            ## Prometheus StorageSpec for persistent data
            ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
            ##
            storageSpec: {}
            #  volumeClaimTemplate:
            #    spec:
            #      storageClassName: gluster
            #      accessModes: ["ReadWriteOnce"]
            #      resources:
            #        requests:
            #          storage: 50Gi
            #    selector: {}

            ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
            ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
            ## as specified in the official Prometheus documentation:
            ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<scrape_config>. As scrape configs are
            ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
            ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
            ## scrape configs are going to break Prometheus after the upgrade.
            ##
            additionalScrapeConfigs:
            - job_name: redis_exporter
              static_configs:
              - targets: ['prometheus-redis-exporter:9121']
              # create new label "deployment" matching with the queue's conumser
              metric_relabel_configs:
              - source_labels: ['key']
                regex: '(^.*$)'
                replacement: '${1}-consumer'
                target_label: deployment

            - job_name: tensorflow
              scrape_interval: 5s
              metrics_path: /monitoring/prometheus/metrics
              static_configs:
                - targets: ['tf-serving.deepcell:8501']
